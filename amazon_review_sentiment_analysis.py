# -*- coding: utf-8 -*-
"""Amazon Review Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SaEni7s13VTQBDi4t5NGB60b01dxZTB-

# Set Up
"""

!pip install ijson

import io
import os
import ijson
import pandas as pd
import numpy as np
import datetime
import json
import re
import seaborn as sns
import matplotlib.pyplot as plt
import string
import itertools
import warnings
warnings.filterwarnings("ignore")

#NLTK
import nltk
from nltk.tokenize import RegexpTokenizer, word_tokenize
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import PorterStemmer

# modeling
from scipy.stats import uniform
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, roc_curve, precision_score, precision_recall_curve, recall_score, auc
import scipy as sp
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score, TimeSeriesSplit
from sklearn.feature_selection import RFE
from sklearn.utils import resample
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, roc_auc_score
from sklearn import svm

nltk.download('stopwords')

nltk.download('punkt')

"""# Data Understanding

## Data Description

Dataset: Amazon product review data (Health and Personal Care)

Data Source: http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Health_and_Personal_Care_5.json.gz

Data Size: 343304 rows * 10 columns

Features: 
1. reviewerID: unique identifyer for each reviewer on Amazon
2. asin: Amazon Standard Identification Number; a 10-charcter alphanumeric unique identifier
3. reviewerName: the name of reviewer
4. helpful_start: the number of helpful votes when started
5. helpful_end: the number of helpful votes when ended
6. reviewText: the text content of review
7. overall: a number between 0-5 indicating how much the reviewer rates the product
8. summary: a summary of the review
9. unixReviewTime: the time reviewer gives the review in Unix
10. reviewTime: the time reviewer gives the review in month, day, year

## Import Dataset
"""

# this helps you connect to the google drive
from google.colab import drive
drive.mount('/content/drive',force_remount=True)

!ls '/content/drive/My Drive/ConFiveDance/453 Team Project/reviews_Health_and_Personal_Care_5.json'

cols = ["reviewerID", "asin", "reviewerName", "helpful_start",
        "helpful_end", "reviewText", "overall", "summary",
        "unixReviewTime", "reviewTime"]

reviewerID = []
asin = []
reviewerName = []
helpful_start = []
helpful_end = []
reviewText = []
overall = []
summary = []
unixReviewTime = []
reviewTime = []
helpful = []

a = datetime.datetime.now()

with open('/content/drive/My Drive/ConFiveDance/reviews_Health_and_Personal_Care_5.json', encoding="UTF-8") as json_file:
    cursor = 0
    for line_number, line in enumerate(json_file):
        #print ("Processing line", line_number + 1,"at cursor index:", cursor)
        line_as_file = io.StringIO(line)
        # Use a new parser for each line
        json_parser = ijson.parse(line_as_file)
        for prefix, type, value in json_parser:
            if (type == 'string') | (type == 'number'):
                if prefix.strip() == 'reviewerID':
                    reviewerID.append(value)
                    #print(value)
                elif prefix.strip() == 'asin':
                    asin.append(value)
                elif prefix.strip() == 'reviewerName':
                    reviewerName.append(value)
                elif prefix.strip() == 'helpful.item':
                    helpful.append(value)
                elif prefix.strip() == 'reviewText':
                    reviewText.append(value)
                elif prefix.strip() == 'overall':
                    overall.append(value)
                elif prefix.strip() == 'summary':
                    summary.append(value)
                elif prefix.strip() == 'unixReviewTime':
                    unixReviewTime.append(value)
                elif prefix.strip() == 'reviewTime':
                    reviewTime.append(value)
        cursor += len(line)

        
b = datetime.datetime.now()
       
helpful_start = [helpful[i] for i in range(len(helpful)) if i%2 == 0]
helpful_end = [helpful[i] for i in range(len(helpful)) if i%2 == 1]

review = pd.DataFrame(list(zip(reviewerID, asin, reviewerName, helpful_start, helpful_end,
                           reviewText, overall, summary, unixReviewTime, reviewTime)),
               columns =cols)
c=b-a
print(c.seconds)

review.shape

review.head(5)

"""## Data Exploration"""

# Make sure the dataset is clean and contains no null values
review.isnull().sum()

# Bar plot showing the distribution of ratings
sns.countplot(review.overall)
plt.xlabel('Rating')
plt.show()
print('Percentage of 5 stars is:', str(round(review['overall'].value_counts()[5]*100/review.shape[0],2)) + '%')
print('Percentage of 4 stars is:', str(round(review['overall'].value_counts()[4]*100/review.shape[0],2)) + '%')

review.groupby('overall').size()

# Find the correlation of every pair of variables
review.corr()

"""# Data Preparation

## Text Processing
"""

reg_data = review[["overall", "reviewText"]]

# create y variables by defining sentiment
def sentiment(overall):
    
    if overall <= 3:
        return 0
    else:
        return 1
    
reg_data['sentiment'] = reg_data['overall'].map(sentiment)

reg_data.head()

def text_processing(sentence):
  # handle negation, turn n't to not
  nt2not = re.sub(r"n't", " not", sentence)
  # turn all the character into lower case
  review_lowercase = nt2not.lower()
  # remove punctuation
  review_no_punc = review_lowercase.translate(str.maketrans('', '', string.punctuation))
  # handle negation, turn not to not_ with the next word
  not2not_ = re.sub(" not ", " not_", review_no_punc)
  return not2not_

reg_data['reviewText'] = reg_data['reviewText'].map(text_processing)

reg_data.head()

# remove stop words and stem all the words
stop_words = set(stopwords.words('english'))
porter = PorterStemmer() # define the stemmer
def rm_stop_stem(sentence):
  word_tokens = word_tokenize(sentence)
  no_stop_stem_word = [porter.stem(w) for w in word_tokens if w not in stop_words  ]  # stem and remove stop words
  return " ".join(no_stop_stem_word) # returning the sentence in the form of a string

reg_data['reviewText'] = reg_data['reviewText'].map(rm_stop_stem)

reg_data.head()

# define the x and y and split train and test data
X = reg_data["reviewText"] 
y = reg_data["sentiment"]
X_train,X_test,Y_train,Y_test = train_test_split(X, y, test_size=0.3, random_state=2)

print("Train Data Size: ",X_train.shape)
print("Test Data Size: ",X_test.shape)

print("Train Data Size: ",Y_train.shape)
print("Test Data Size: ",Y_test.shape)

"""## Upsampling
Since we have unbalanced data: approximately 80% of the labels are positive sentiment, whereas the negative class only takes up 20% of the data.
"""

from sklearn.utils import resample
X_Y_train = pd.concat([X_train, Y_train], ignore_index=True, axis=1).reset_index(drop=True)
X_Y_train.columns = ["review", "sentiment"]
neg_senti = X_Y_train[X_Y_train.sentiment==0]
pos_senti = X_Y_train[X_Y_train.sentiment==1]
neg_senti_up = resample(neg_senti,
                          replace=True, # sample with replacement
                          n_samples=len(pos_senti), # match number in majority class
                          random_state=27)
upsampled_train = pd.concat([pos_senti, neg_senti_up], ignore_index=True, axis=0)

X_train_up = upsampled_train["review"]
Y_train_up = upsampled_train["sentiment"]

# take a sample of 10000 for the SVM model, otherwise it takes forever to run or can't even train a model
# just to demonstrate how we train a SVM model
upsampled_svm=upsampled_train.sample(n=10000, replace=False, random_state=22)

X_train_up_svm=upsampled_svm["review"]
Y_train_up_svm=upsampled_svm["sentiment"]

test_full = pd.concat([X_test, Y_test], axis=1, ignore_index=True).reset_index(drop=True)
test_full.columns = ["review", "sentiment"]
test_full.head()

test_svm = test_full.sample(n=3000, replace=False, random_state=22)

X_test_svm = test_svm["review"]
Y_test_svm = test_svm["sentiment"]

print(len(X_test_svm))
print(len(Y_test_svm))

"""# 2. Text Vectorization

## Bag of Words
"""

bow_vect = CountVectorizer()

X_train_bow = bow_vect.fit_transform(X_train_up)
X_test_bow = bow_vect.transform(X_test)

X_train_bow.shape

# take a smaller number of features for the SVM model
bow_vect_svm = CountVectorizer(max_features=100)
X_train_svm_bow = bow_vect_svm.fit_transform(X_train_up_svm)
X_test_svm_bow = bow_vect_svm.transform(X_test_svm)

"""## TF-IDF"""

vectorizer = TfidfVectorizer(min_df = 5)

X_train_tfidf = vectorizer.fit_transform(X_train_up)
X_test_tfidf = vectorizer.transform(X_test)

X_train_tfidf.shape

# take a smaller number of features for the SVM model
tfidf_vect_svm = CountVectorizer(max_features=100)
X_train_svm_tfidf = tfidf_vect_svm.fit_transform(X_train_up_svm)
X_test_svm_tfidf = tfidf_vect_svm.transform(X_test_svm)

"""## N-Grams"""

# n-gram vectorization: taking 1- and 2-grams, maximum 10000 features
ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2), max_features=10000)
ngram_vectorizer.fit(X_train_up)
X_train_ngrams = ngram_vectorizer.transform(X_train_up)
X_test_ngrams = ngram_vectorizer.transform(X_test)

# take a smaller number of features for the SVM model
ngram_vect_svm = CountVectorizer(max_features=100)
X_train_svm_ngram = ngram_vect_svm.fit_transform(X_train_up_svm)
X_test_svm_ngram = ngram_vect_svm.transform(X_test_svm)

"""# 3-5(a). Modeling & Evaluation: Sentiment Analysis Predictive Models, GridSearchCV Hyperparameter Tuning, and Model Performances

## Logistic Regression

### Bag of Words
"""

# fit a logistic regression model
logreg = LogisticRegression()
params = {
    'penalty': ['l2', 'elasticnet', 'none'],
    'C': [0.0001, 0.001, 0.01, 0.1],
    'random_state': [42]
}

# use GridSearchCV to choose hyperparameters
grid_logreg = GridSearchCV(logreg, params, scoring='f1', cv=4, n_jobs=2)
grid_logreg.fit(X_train_bow, Y_train_up)

grid_logreg.best_params_

# calculate the model selection scores
Y_pred = grid_logreg.predict(X_test_bow)
conf_mat = confusion_matrix(Y_test, Y_pred)
accuracy = accuracy_score(Y_test, Y_pred)
precision = precision_score(Y_test, Y_pred)
recall = recall_score(Y_test, Y_pred)
auc = roc_auc_score(Y_test, Y_pred)

print('Logistic Regressionos Model Accuracy score: {0:0.3f}'.format(accuracy))
print('Logistic Regressionos Model Precision score: {0:0.3f}'.format(precision))
print('Logistic Regressionos Model Recall score: {0:0.3f}'.format(recall))
print('Logistic Regressionos Model AUC score: {0:0.3f}'.format(auc))

#confusion matrix
cm=confusion_matrix(Y_test,Y_pred)
cm

"""### TF-IDF"""

# fit a logistic regression model
logreg = LogisticRegression()
params = {
    'penalty': ['l2', 'elasticnet', 'none'],
    'C': [0.0001, 0.001, 0.01, 0.1],
    'random_state': [42]
}

# use GridSearchCV to choose hyperparameters
grid_logreg = GridSearchCV(logreg, params, scoring='f1', cv=4, n_jobs=2)
grid_logreg.fit(X_train_tfidf, Y_train_up)

grid_logreg.best_params_

# calculate the model selection scores
Y_pred = grid_logreg.predict(X_test_tfidf)
conf_mat = confusion_matrix(Y_test, Y_pred)
accuracy = accuracy_score(Y_test, Y_pred)
precision = precision_score(Y_test, Y_pred)
recall = recall_score(Y_test, Y_pred)
auc = roc_auc_score(Y_test, Y_pred)

print('Logistic Regressionos Model Accuracy score: {0:0.3f}'.format(accuracy))
print('Logistic Regressionos Model Precision score: {0:0.3f}'.format(precision))
print('Logistic Regressionos Model Recall score: {0:0.3f}'.format(recall))
print('Logistic Regressionos Model AUC score: {0:0.3f}'.format(auc))

"""### N-Grams"""

# fit a logistic regression model
logreg = LogisticRegression()
params = {
    'penalty': ['l2', 'elasticnet', 'none'],
    'C': [0.0001, 0.001, 0.01, 0.1],
    'random_state': [42]
}

# use GridSearchCV to choose hyperparameters
grid_logreg = GridSearchCV(logreg, params, scoring='f1', cv=4, n_jobs=2)
grid_logreg.fit(X_train_ngrams, Y_train_up)

grid_logreg.best_params_

#confusion matrix
cm=confusion_matrix(Y_test,Y_pred)
cm

# calculate the model selection scores
Y_pred = grid_logreg.predict(X_test_ngrams)
conf_mat = confusion_matrix(Y_test, Y_pred)
accuracy = accuracy_score(Y_test, Y_pred)
precision = precision_score(Y_test, Y_pred)
recall = recall_score(Y_test, Y_pred)
auc = roc_auc_score(Y_test, Y_pred)

print('Logistic Regressionos Model Accuracy score: {0:0.3f}'.format(accuracy))
print('Logistic Regressionos Model Precision score: {0:0.3f}'.format(precision))
print('Logistic Regressionos Model Recall score: {0:0.3f}'.format(recall))
print('Logistic Regressionos Model AUC score: {0:0.3f}'.format(auc))

# this is the best model given the highest ROC AUC score and accuracy

"""## Random Forest

### Bag of Words
"""

# fit a random forest model
rf = RandomForestClassifier()

params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [4, 6, 10, 12],
    'random_state': [13]
}

grid_rf = GridSearchCV(rf, param_grid=params, scoring='f1', n_jobs=-1, cv=4)
grid_rf.fit(X_train_bow, Y_train_up)

grid_rf.best_params_

# calculate the model selection scores
Y_pred = grid_rf.predict(X_test_bow)
conf_mat = confusion_matrix(Y_test, Y_pred)
accuracy = accuracy_score(Y_test, Y_pred)
precision = precision_score(Y_test, Y_pred)
recall = recall_score(Y_test, Y_pred)
auc = roc_auc_score(Y_test, Y_pred)

print('Random Forest Model Confusion Matrix:\n', conf_mat)
print('Random Forest Model Accuracy score: {0:0.3f}'.format(accuracy))
print('Random Forest Model Precision score: {0:0.3f}'.format(precision))
print('Random Forest Model Recall score: {0:0.3f}'.format(recall))
print('Random Forest Model AUC score: {0:0.3f}'.format(auc))

#confusion matrix
cm=confusion_matrix(Y_test,Y_pred)
cm

"""### TF-IDF"""

# fit a random forest model
rf = RandomForestClassifier()

params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [4, 6, 10, 12],
    'random_state': [13]
}

grid_rf = GridSearchCV(rf, param_grid=params, scoring='f1', n_jobs=-1, cv=4)
grid_rf.fit(X_train_tfidf, Y_train_up)

grid_rf.best_params_

# calculate the model selection scores
Y_pred = grid_rf.predict(X_test_tfidf)
conf_mat = confusion_matrix(Y_test, Y_pred)
accuracy = accuracy_score(Y_test, Y_pred)
precision = precision_score(Y_test, Y_pred)
recall = recall_score(Y_test, Y_pred)
auc = roc_auc_score(Y_test, Y_pred)
print('Random Forest Model Accuracy score: {0:0.3f}'.format(accuracy))
print('Random Forest Model Precision score: {0:0.3f}'.format(precision))
print('Random Forest Model Recall score: {0:0.3f}'.format(recall))
print('Random Forest Model AUC score: {0:0.3f}'.format(auc))

#confusion matrix
cm=confusion_matrix(Y_test,Y_pred)
cm

"""### N-Grams"""

# fit a random forest model
rf = RandomForestClassifier()

params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [4, 6, 10, 12],
    'random_state': [13]
}

grid_rf = GridSearchCV(rf, param_grid=params, scoring='f1', n_jobs=-1, cv=4)
grid_rf.fit(X_train_ngrams, Y_train_up)

grid_rf.best_params_

# calculate the model selection scores
Y_pred = grid_rf.predict(X_test_ngrams)
conf_mat = confusion_matrix(Y_test, Y_pred)
accuracy = accuracy_score(Y_test, Y_pred)
precision = precision_score(Y_test, Y_pred)
recall = recall_score(Y_test, Y_pred)
auc = roc_auc_score(Y_test, Y_pred)
print('Random Forest Model Accuracy score: {0:0.3f}'.format(accuracy))
print('Random Forest Model Precision score: {0:0.3f}'.format(precision))
print('Random Forest Model Recall score: {0:0.3f}'.format(recall))
print('Random Forest Model AUC score: {0:0.3f}'.format(auc))

#confusion matrix
cm=confusion_matrix(Y_test,Y_pred)
cm

"""## SVM
In this section, we used the smaller samples we took for SVM models for demonstration purposes, otherwise the SVM model takes forever to be trained for such a large dataset.

### Bag of Words
"""

from random import sample

# fit a SVC model
classifier = svm.SVC()
params = [{'kernel': ['rbf', 'sigmoid'], 'gamma': [1e-3, 1e-4],
           'C': [0.0001, 0.001, 0.01, 0.1]}]
# use GridSearchCV to select hyperparameters
grid_svm = GridSearchCV(classifier, param_grid=params, scoring='f1', cv=4, n_jobs=-1)
grid_svm.fit(X_train_svm_bow, Y_train_up_svm)

grid_svm.best_params_

# calculate the model selection scores
Y_pred = grid_svm.predict(X_test_svm_bow)
conf_mat = confusion_matrix(Y_test_svm, Y_pred)
accuracy = accuracy_score(Y_test_svm, Y_pred)
precision = precision_score(Y_test_svm, Y_pred)
recall = recall_score(Y_test_svm, Y_pred)
auc = roc_auc_score(Y_test_svm, Y_pred)

print('SVM Confusion Matrix:\n', conf_mat)
print('SVM Accuracy score: {0:0.3f}'.format(accuracy))
print('SVM Precision score: {0:0.3f}'.format(precision))
print('SVM Recall score: {0:0.3f}'.format(recall))
print('SVM AUC score: {0:0.3f}'.format(auc))

"""### TF-IDF"""

# fit a SVC model
classifier = svm.SVC()
params = [{'kernel': ['rbf', 'sigmoid'], 'gamma': [1e-3, 1e-4],
           'C': [0.0001, 0.001, 0.01, 0.1]}]
# use GridSearchCV to select hyperparameters
grid_svm = GridSearchCV(classifier, param_grid=params, scoring='f1', cv=4, n_jobs=-1)
grid_svm.fit(X_train_svm_tfidf, Y_train_up_svm)

grid_svm.best_params_

# calculate the model selection scores
Y_pred = grid_svm.predict(X_test_svm_tfidf)
conf_mat = confusion_matrix(Y_test_svm, Y_pred)
accuracy = accuracy_score(Y_test_svm, Y_pred)
precision = precision_score(Y_test_svm, Y_pred)
recall = recall_score(Y_test_svm, Y_pred)
auc = roc_auc_score(Y_test_svm, Y_pred)

print('SVM Confusion Matrix:\n', conf_mat)
print('SVM Accuracy score: {0:0.3f}'.format(accuracy))
print('SVM Precision score: {0:0.3f}'.format(precision))
print('SVM Recall score: {0:0.3f}'.format(recall))
print('SVM AUC score: {0:0.3f}'.format(auc))

"""### N-Grams"""

# fit a SVC model
classifier = svm.SVC()
params = [{'kernel': ['rbf', 'sigmoid'], 'gamma': [1e-3, 1e-4],
           'C': [0.0001, 0.001, 0.01, 0.1]}]
# use GridSearchCV to select hyperparameters
grid_svm = GridSearchCV(classifier, param_grid=params, scoring='f1', cv=4, n_jobs=-1)
grid_svm.fit(X_train_svm_ngram, Y_train_up_svm)

grid_svm.best_params_

# calculate the model selection scores
Y_pred = grid_svm.predict(X_test_svm_ngram)
conf_mat = confusion_matrix(Y_test_svm, Y_pred)
accuracy = accuracy_score(Y_test_svm, Y_pred)
precision = precision_score(Y_test_svm, Y_pred)
recall = recall_score(Y_test_svm, Y_pred)
auc = roc_auc_score(Y_test_svm, Y_pred)

print('SVM Confusion Matrix:\n', conf_mat)
print('SVM Accuracy score: {0:0.3f}'.format(accuracy))
print('SVM Precision score: {0:0.3f}'.format(precision))
print('SVM Recall score: {0:0.3f}'.format(recall))
print('SVM AUC score: {0:0.3f}'.format(auc))